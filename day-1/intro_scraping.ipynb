{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to web-scraping\n",
    "\n",
    "It's 2019. The web is everywhere.\n",
    "\n",
    "* If you want to buy a house, real estate agents have [websites](https://www.wendytlouie.com/) where they list the houses they're currently selling. \n",
    "* If you want to know whether to where a rain jacket or shorts, you check the weather on a [website](https://weather.com/weather/tenday/l/Berkeley+CA+USCA0087:1:US). \n",
    "* If you want to know what's happening in the world, you read the news [online](https://www.sfchronicle.com/). \n",
    "* If you've forgotten which city is the capital of Australia, you check [Wikipedia](https://en.wikipedia.org/wiki/Australia).\n",
    "\n",
    "**The point is this: there is an enormous amount of information (also known as data) on the web.**\n",
    "\n",
    "If we (in our capacities as, for example, data scientists, social scientists, digital humanists, businesses, public servants or members of the public) can get our hands on this information, **we can answer all sorts of interesting questions or solve important problems**.\n",
    "\n",
    "* Maybe you're studying gender bias in student evaluations of professors. One option would be to scrape ratings from [Rate My Professors](https://www.ratemyprofessors.com/) (provided you follow their [terms of service](https://www.ratemyprofessors.com/TermsOfUse_us.jsp#use))\n",
    "* Perhaps you want to build an app that shows users articles relating to their specified interests. You could scrape stories from various news websites and then use NLP methods to decide which articles to show which users.\n",
    "* [Geoff Boeing](https://geoffboeing.com/) and [Paul Waddell](https://ced.berkeley.edu/ced/faculty-staff/paul-waddell) recently published [a great study](https://arxiv.org/pdf/1605.05397.pdf) of the US housing market by scraping millions of Craiglist rental listings. Among other insights, their study shows which metropolitan areas in the US are more or less affordable to renters.\n",
    "\n",
    "This first day's workshop is a one-hour beginner's introduction to web scraping. \n",
    "\n",
    "\n",
    "## Learning Goals\n",
    "*   \n",
    "\n",
    "## Outline\n",
    "\n",
    "* [How the web works](#mechanics)\n",
    "* [Structured queries with APIs](#apis)\n",
    "* [Domain collection with automated google search](#domain)\n",
    "* [Downloading and mirroring with `wget`](#wget)\n",
    "* [6](#6)\n",
    "* [7](#7)\n",
    "* [8](#8)\n",
    "* [9](#9)\n",
    "* [Terms of Service](#terms)\n",
    "\n",
    "## Background\n",
    "\n",
    "We will do some review, but this notebook assumes you have basic familiarity with Python. If you need a beginner's introduction to coding in Python, please walk through the intro to Python notebook at `solutions/intro-to-python.ipynb` and/or [this one](https://github.com/lknelson/text-analysis-course/blob/master/scripts/01.25.02_PythonBasics.ipynb) *before* the workshop. \n",
    "\n",
    "We will also use some regular expressions, which are character sequences defining a search pattern. Usually this pattern is then used by string searching algorithms for \"find\" or \"find and replace\" operations. Don't worry if you haven't seen these before--we will keep it simple. If you want to get more out of this session, first go through [this notebook on regular expressions](https://github.com/lknelson/text-analysis-course/blob/master/scripts/03.20.01_RegularExpressions.ipynb).\n",
    "\n",
    "## Vocabulary\n",
    "\n",
    "* *domain*: \n",
    "    *  \n",
    "* *web-scraping* (i.e., *screen-scraping*):\n",
    "    * Extracting structured information from web pages, usually relying on their HTML or CSS formatting.\n",
    "* *web-crawling*:\n",
    "    * Finding web pages through links, automated search, etc. to download or mirror them.\n",
    "* *downloading*:\n",
    "    *  \n",
    "* *mirroring*:\n",
    "    *  \n",
    "* *Application Programming Interface (API)*:\n",
    "    *  \n",
    "\n",
    "**__________________________________**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How the web works<a id='mechanics'></a>\n",
    "\n",
    "Here's our high-level description of the web.\n",
    "\n",
    "**The internet is a bunch of computers connected together.** Some computers are laptops, some are desktops, some are smart phones, some are servers owned by companies. Each computer has its own address on the internet. Using these addresses, **one computer can ask another computer for some information (data). We say that the first computer sends a _request_ to the second computer, asking for some particular information. The second computer sends back a _response_**. The response could include the information requested, or it could be an error message. Perhaps the second computer doesn't have that information any more, or the first computer isn't allowed to access that information.\n",
    "\n",
    "<img src='../assets/computer-network.png' />\n",
    "\n",
    "We said that there is an enormous amount of information available on the web. When people put information on the web, they generally have two different audiences in mind, two different types of consumers of their information: humans and computers. If they want their information to be used primarily by humans, they'll make a website. This will let them lay out the information in a visually appealing way, choose colours, add pictures, and make the information interactive. If they want their information to be used by computers, they'll make a web API. A web API provides other computers structured access to their data. We won't cover APIs in this workshop, but you should know that i) APIs are very common and ii) if there is an API for a website/data source, you should use that over web scraping. Many data sources that you might be interested in (e.g. social media sites) have APIs.\n",
    "\n",
    "**Websites are just a bunch of files on one of those computers. They are just plain text files, so you can view them if you want. When you type in the address of a website in your browser, your computer sends a request to the computer located at that address. The request says \"hey buddy, please send me the file(s) for this website\". If everything goes well, the other computer will send back the file(s) in the response**. Everytime you navigate to a new website or page in your browser, this process repeats.\n",
    "\n",
    "<img src='../assets/request-response.png' />\n",
    "\n",
    "**There are three main languages that that website files are written with: HyperText Markup Language (HTML), Cascading Style Sheets (CSS) and JavaScript (JS)**. They normally have `.html`, `.css` and `.js` file extensions. Each language (and thus each type of file) serves a different purpose. **HTML files are the ones we care about the most, because they are the ones that contain the text you see on a web page**. CSS files contain the instructions on how to make the content in a HTML visually appealing (all the colours, font sizes, border widths, etc.). JavaScript files have the instructions on how to make the information on a website interactive (things like changing colour when you click something, entering data in a form). In this workshop, we're going to focus on HTML.\n",
    "\n",
    "\n",
    "**It's not too much of a simplification to say:**\n",
    "\n",
    "\\begin{equation}\n",
    "\\textrm{Web scraping} = \\textrm{Making a request for a HTML file} + \\textrm{Parsing the HTML response}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Existing data\n",
    "\n",
    "**Before web scraping, see if you can get the same data elsewhere.** This will often be easier for you and preferred by the people who own the data.\n",
    "\n",
    "For example, Wikipedia offers an [API](https://www.mediawiki.org/wiki/REST_API) to access their pages. In fact, Wikipedia would prefer that we access their data that way rather than web-scraping. There's even a [Python package](https://pypi.org/project/wikipedia/) that wraps around this API to make it even easier to use. Wikipedia also makes all of its content available for [direct download](https://dumps.wikimedia.org/). \n",
    "\n",
    "Moreover, if you're affiliated with an institution, you may be breaching existing contracts by engaging in scraping. UC Berkeley's Library [recommends](http://guides.lib.berkeley.edu/text-mining) following this workflow:\n",
    "\n",
    "<img src='../assets/workflow.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Terms of Service<a id='terms'></a>\n",
    "\n",
    "As you've seen, web scraping involves making requests from other computers for their data. It costs people money to maintain the computers that we request data from: it needs electricity, it requires staff, sometimes you need to upgrade the computer, etc. But we didn't pay anyone for using their resources.\n",
    "\n",
    "Because we're making these requests programmatically, we could make many, many requests per second. For example, we could put a request in a never-ending loop which would constantly request data from a server. But computers can't handle too much traffic, so eventually this might crash someone else's computer. Moreover, if we make too many requests when we're web scraping, that might restrict the number of people who can view the web page in their browser. This isn't very nice.\n",
    "\n",
    "Websites often have Terms of Service, documents that you agree to whenever you visit a site. Some of these terms prohibit web scraping, because it puts too much strain on their servers, or they just don't want their data accessed programmatically. Whatever the reason, we need to respect a websites Terms of Service. **Before you scrape a site, you should always check its terms of service to make sure it's allowed.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structured queries with APIs<a id='apis'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, let's try out the [Google Fact Check API](https://developers.google.com/fact-check/tools/api/), which can be easily explored [in a browser](https://toolbox.google.com/factcheck/explorer). By searching this Google service, the Fact Identifier collects facts relevant to the query input by user (or built in by default, as in the current version)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "import requests # for downloading\n",
    "from bs4 import BeautifulSoup, NavigableString, Tag # for html scraping\n",
    "import regex as re # Regex module with Unicode support\n",
    "import html5lib # slower but more accurate bs4 parser for messy HTML # lxml faster\n",
    "import urllib\n",
    "import json\n",
    "\n",
    "# Import functions to scrape fact check web pages\n",
    "from scrape_helpers import load_api_key, clean_text, scrape_politifact, scrape_factcheck, scrape_snopes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################\n",
    "# Call API\n",
    "######################################################\n",
    "\n",
    "# Elements in query response: text, claimDate, claimReview[publisher[name], url, textualRating]\n",
    "# Columns in output CSV: (date (DD-MM-YYYY), claim, truth rating, url, source (publisher), fact, explanation\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    page_token = 0\n",
    "    domains = ['covid', 'blm', 'election']\n",
    "    query_sets = [\n",
    "        [\"masks\", \"Chinese bioweapon\", \"China virus\"],\n",
    "        [\"George Floyd\", \"Antifa\", \"Black Lives Matter\"],\n",
    "        [\"Hunter Biden\", \"rigged election\", \"mail-in\", \"election ballots\"],\n",
    "    ]\n",
    "\n",
    "    api_key_fp = \"api_key.txt\"\n",
    "    key = load_api_key(api_key_fp)\n",
    "    endpoint = 'https://factchecktools.googleapis.com'\n",
    "    search = '/v1alpha1/claims:search'\n",
    "\n",
    "    sites = ['politifact.com', 'factcheck.org', 'snopes.com']\n",
    "    site_scrapers = [scrape_politifact, scrape_factcheck, scrape_snopes]\n",
    "    site_switches = ['politifact', 'factcheck.org', 'snopes']\n",
    "\n",
    "\n",
    "    for i in range(0, len(domains)):\n",
    "        domain = domains[i]\n",
    "        queries = query_sets[i]\n",
    "        claims = [] # initialize list of claims\n",
    "\n",
    "        for query in queries:\n",
    "            urls = set() # initialize set of fact check URLs already seen for this query\n",
    "\n",
    "            for site in tqdm(sites, desc='Collecting data for {} via API'.format(query)):\n",
    "                params = {\n",
    "                    'pageToken': page_token,\n",
    "                    'query': query,\n",
    "                    'reviewPublisherSiteFilter': site,\n",
    "                    'key': key\n",
    "                }\n",
    "\n",
    "                nextToken = True\n",
    "                while nextToken:\n",
    "                    url = endpoint + search + '?' + urllib.parse.urlencode(params)\n",
    "                    response = requests.get(url)\n",
    "                    data = response.json()\n",
    "\n",
    "                    if 'claims' in data:\n",
    "                        for claim in data['claims']:\n",
    "                            if not site == 'snopes.com':\n",
    "                                claims.append([claim['claimDate'],\n",
    "                                               claim['text'],\n",
    "                                               claim['claimReview'][0]['textualRating'],\n",
    "                                               claim['claimReview'][0]['url'],\n",
    "                                               claim['claimReview'][0]['publisher']['name']])\n",
    "                            else:\n",
    "                                claims.append([claim['claimReview'][0]['reviewDate'],\n",
    "                                               claim['text'],\n",
    "                                               claim['claimReview'][0]['textualRating'],\n",
    "                                               claim['claimReview'][0]['url'],\n",
    "                                               claim['claimReview'][0]['publisher']['name']\n",
    "                                              .replace('.com', '')])\n",
    "\n",
    "                    if 'nextPageToken' in data:\n",
    "                        params['pageToken'] = data['nextPageToken']\n",
    "                    else:\n",
    "                        nextToken = False\n",
    "\n",
    "            for j in tqdm(range(0, len(claims)), desc='Scraping websites'.format(query)):\n",
    "                claim = claims[j]\n",
    "                switch = site_switches.index(claim[4].lower()) # use fact to get publisher site then index (site name is 5th element)\n",
    "                scraper = site_scrapers[switch] # get scraper using index\n",
    "                claim.extend(scraper(claim[3])) # scrape URL using scraper (URL is 4th element), add to existing claim info\n",
    "                claims[j] = claim # record fact in list\n",
    "\n",
    "            claims # remove duplicates\n",
    "\n",
    "            # Save output for this query\n",
    "            query_string = query.replace(' ', '-')\n",
    "            with open('data/fact_checker_data_{}.csv'.format(query_string), 'w') as f:\n",
    "                csv_writer = csv.writer(f, delimiter=',', quoting=csv.QUOTE_MINIMAL)\n",
    "                csv_writer.writerow(['date', 'claim', 'truth_rating', 'url', 'source', 'fact', 'explanation'])\n",
    "                for claim in claims: # Each claim gets its own column\n",
    "                    if claim[3] not in urls: # don't add if fact check URL already seen\n",
    "                        csv_writer.writerow(claim) # save row\n",
    "                        urls.add(claim[3]) # add to set of urls already saved\n",
    "\n",
    "            print('Saved {} claims for {} query.'.format(str(len(urls)), query))\n",
    "            print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Domain collection with automated google search<a id='domain'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script uses two related functions to scrape the best URL from online sources: \n",
    "> The Google Places API. See the [GitHub page](https://github.com/slimkrazy/python-google-places) for the Python wrapper and sample code, [Google Web Services](https://developers.google.com/places/web-service/) for general documentation, and [here](https://developers.google.com/places/web-service/details) for details on Place Details requests.\n",
    "\n",
    "> The Google Search function (manually filtered). See [here](https://pypi.python.org/pypi/google) for source code and [here](http://pythonhosted.org/google/) for documentation.\n",
    "\n",
    "To get an API key for the Google Places API (or Knowledge Graph API), go to the [Google API Console](http://code.google.com/apis/console).\n",
    "To upgrade your quota limits, sign up for billing--it's free and raises your daily request quota from 1K to 150K (!!).\n",
    "\n",
    "The code below doesn't use Google's Knowledge Graph (KG) Search API because this turns out NOT to reveal websites related to search results (despite these being displayed in the KG cards visible at right in a standard Google search). The KG API is only useful for scraping KG id, description, name, and other basic/ irrelevant info. TO see examples of how the KG API constructs a search URL, etc., (see [here](http://searchengineland.com/cool-tricks-hack-googles-knowledge-graph-results-featuring-donald-trump-268231)).\n",
    "\n",
    "Possibly useful note on debugging: An issue causing the GooglePlaces package to unnecessarily give a \"ValueError\" and stop was resolved in [July 2017](https://github.com/slimkrazy/python-google-places/issues/59). <br>\n",
    "Other instances of this error may occur if Google Places API cannot identify a location as given. Dealing with this is a matter of proper Exception handling (which seems to be working fine below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install google # For automated Google searching \n",
    "!pip install https://github.com/slimkrazy/python-google-places/zipball/master # Google Places API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dicts_to_csv(list_of_dicts, file_name, header):\n",
    "    '''This helper function writes a list of dictionaries to a csv called file_name, with column names decided by 'header'.'''\n",
    "    \n",
    "    with open(file_name, 'w') as output_file:\n",
    "        print(\"Saving to \" + str(file_name) + \" ...\")\n",
    "        dict_writer = csv.DictWriter(output_file, header)\n",
    "        dict_writer.writeheader()\n",
    "        dict_writer.writerows(list_of_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_left(list_of_dicts, varname):\n",
    "    '''This helper function determines how many dicts in list_of_dicts don't have a valid key/value pair with key varname.'''\n",
    "    \n",
    "    count = 0\n",
    "    for school in list_of_dicts:\n",
    "        if school[varname] == \"\" or school[varname] == None:\n",
    "            count += 1\n",
    "\n",
    "    print(str(count) + \" schools in this data are missing \" + str(varname) + \"s.\")\n",
    "\n",
    "count_left(sample, 'URL')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Python search environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTING KEY PACKAGES\n",
    "from googlesearch import search # automated Google Search package\n",
    "from googleplaces import GooglePlaces, types, lang  # Google Places API\n",
    "\n",
    "import csv, re, os  # Standard packages\n",
    "import pandas as pd  # for working with csv files\n",
    "import urllib, requests  # for scraping\n",
    "from tqdm import tqdm # for progress tracking in for loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing Google Places API search functionality\n",
    "places_api_key = re.sub(\"\\n\", \"\", open(\"../data/places_api_key.txt\").read())\n",
    "print(places_api_key)\n",
    "\n",
    "google_places = GooglePlaces(places_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here's a list of sites we DON'T want to spider, \n",
    "# but that an automated Google search might return...\n",
    "# and we might thus accidentally spider unless we filter them out (as below)!\n",
    "\n",
    "bad_sites = []\n",
    "with open('../data/bad_sites.csv', 'r', encoding = 'utf-8') as csvfile:\n",
    "    for row in csvfile:\n",
    "        bad_sites.append(re.sub('\\n', '', row))\n",
    "\n",
    "print(bad_sites)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the Google Places API wrapper at work!\n",
    "school_name = \"River City Scholars Charter Academy\"\n",
    "address = \"944 Evergreen Street, Grand Rapids, MI 49507\"\n",
    "\n",
    "query_result = google_places.nearby_search(\n",
    "        location=address, name=school_name,\n",
    "        radius=15000, types=[types.TYPE_SCHOOL], rankby='distance')\n",
    "\n",
    "for place in query_result.places:\n",
    "    print(place.name)\n",
    "    place.get_details()  # makes further API call\n",
    "    #print(place.details) # A dict matching the JSON response from Google.\n",
    "    print(place.website)\n",
    "    print(place.formatted_address)\n",
    "\n",
    "# Are there any additional pages of results?\n",
    "if query_result.has_next_page_token:\n",
    "    query_result_next_page = google_places.nearby_search(\n",
    "            pagetoken=query_result.next_page_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of using the google search function:\n",
    "for url in search('DR DAVID C WALKER INT 6500 IH 35 N STE C, SAN ANTONIO, TX 78218', \\\n",
    "                  stop=5, pause=5.0):\n",
    "    print(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = []  # make empty list in which to store the dictionaries\n",
    "\n",
    "if os.path.exists('../data/filtered_schools.csv'):  # first, check if file containing search results is available on disk\n",
    "    file_path = '../data/filtered_schools.csv'\n",
    "else:  # use original data if no existing results are available on disk\n",
    "    file_path = '../../data_management/data/charters_unscraped_noURL_2015.csv'\n",
    "\n",
    "with open(file_path, 'r', encoding = 'utf-8') as csvfile: # open file                      \n",
    "    print('Reading in ' + str(file_path) + ' ...')\n",
    "    reader = csv.DictReader(csvfile)  # create a reader\n",
    "    for row in reader:  # loop through rows\n",
    "        sample.append(row)  # append each row to the list\n",
    "\n",
    "print(\"\\nColumns in data: \")\n",
    "print(list(sample[0]))\n",
    "sample = sample[0:5]\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new \"URL\" and \"NUM_BAD_URLS\" variables for each school, without overwriting any with data there already:\n",
    "for school in sample:\n",
    "    try:\n",
    "        if len(school[\"URL\"]) > 0:\n",
    "            pass\n",
    "        \n",
    "    except (KeyError, NameError):\n",
    "        school[\"URL\"] = \"\"\n",
    "\n",
    "for school in sample:\n",
    "    try:\n",
    "        if school[\"QUERY_RANKING\"]:\n",
    "            pass\n",
    "        \n",
    "    except (KeyError, NameError):\n",
    "        school[\"QUERY_RANKING\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Take a look at the first entry's contents and the variables list in our sample (a list of dictionaries)\n",
    "print(sample[1][\"SCH_NAME\"], \"\\n\", sample[1][\"ADDRESSES\"], \"\\n\", sample[1][\"NCESSCH\"], \"\\n\")\n",
    "print(sample[1].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getURL(school_name, address, bad_sites_list): # manual_url\n",
    "    \n",
    "    '''This function finds the one best URL for a school using two methods:\n",
    "    \n",
    "    1. If a school with this name can be found within 20 km (to account for proximal relocations) in\n",
    "    the Google Maps database (using the Google Places API), AND\n",
    "    if this school has a website on record, then this website is returned.\n",
    "    If no school is found, the school discovered has missing data in Google's database (latitude/longitude, \n",
    "    address, etc.), or the address on record is unreadable, this passes to method #2. \n",
    "    \n",
    "    2. An automated Google search using the school's name + address. This is an essential backup plan to \n",
    "    Google Places API, because sometimes the address on record (courtesy of Dept. of Ed. and our tax dollars) is not \n",
    "    in Google's database. For example, look at: \"3520 Central Pkwy Ste 143 Mezz, Cincinnati, OH 45223\". \n",
    "    No wonder Google Maps can't find this. How could it intelligibly interpret \"Mezz\"?\n",
    "    \n",
    "    Whether using the first or second method, this function excludes URLs with any of the 62 bad_sites defined above, \n",
    "    e.g. trulia.com, greatschools.org, mapquest. It returns the number of excluded URLs (from either method) \n",
    "    and the first non-bad URL discovered.'''\n",
    "    \n",
    "    \n",
    "    ## INITIALIZE\n",
    "    \n",
    "    new_urls = []    # start with empty list\n",
    "    good_url = \"\"    # output goes here\n",
    "    k = 0    # initialize counter for number of URLs skipped\n",
    "    \n",
    "    radsearch = 15000  # define radius of Google Places API search, in km\n",
    "    numgoo = 20  # define number of google results to collect for method #2\n",
    "    wait_time = 20.0  # define length of pause between Google searches (longer is better for big catches like this)\n",
    "    \n",
    "    search_terms = school_name + \" \" + address\n",
    "    print(\"Getting URL for \" + school_name + \", \" + address + \"...\")    # show school name & address\n",
    "    \n",
    "    \n",
    "    \n",
    "    ## FIRST URL-SCRAPE ATTEMPT: GOOGLE PLACES API\n",
    "    # Search for nearest school with this name within radsearch km of this address\n",
    "    \n",
    "    try:\n",
    "        query_result = google_places.nearby_search(\n",
    "            location=address, name=school_name,\n",
    "            radius=radsearch, types=[types.TYPE_SCHOOL], rankby='distance')\n",
    "        \n",
    "        for place in query_result.places:\n",
    "            place.get_details()  # Make further API call to get detailed info on this place\n",
    "\n",
    "            found_name = place.name  # Compare this name in Places API to school's name on file\n",
    "            found_address = place.formatted_address  # Compare this address in Places API to address on file\n",
    "\n",
    "            try: \n",
    "                url = place.website  # Grab school URL from Google Places API, if it's there\n",
    "\n",
    "                if any(domain in url for domain in bad_sites_list):\n",
    "                    k+=1    # If this url is in bad_sites_list, add 1 to counter and move on\n",
    "                    #print(\"  URL in Google Places API is a bad site. Moving on.\")\n",
    "\n",
    "                else:\n",
    "                    good_url = url\n",
    "                    print(\"    Success! URL obtained from Google Places API with \" + str(k) + \" bad URLs avoided.\")\n",
    "                    \n",
    "                    '''\n",
    "                    # For testing/ debugging purposes:\n",
    "                    \n",
    "                    print(\"  VALIDITY CHECK: Is the discovered URL of \" + good_url + \\\n",
    "                          \" consistent with the known URL of \" + manual_url + \" ?\")\n",
    "                    print(\"  Also, is the discovered name + address of \" + found_name + \" \" + found_address + \\\n",
    "                          \" consistent with the known name/address of: \" + search_terms + \" ?\")\n",
    "                    \n",
    "                    if manual_url != \"\":\n",
    "                        if manual_url == good_url:\n",
    "                            print(\"    Awesome! The known and discovered URLs are the SAME!\")\n",
    "                    '''\n",
    "                            \n",
    "                    return(k, good_url)  # Returns valid URL of the Place discovered in Google Places API\n",
    "        \n",
    "            except:  # No URL in the Google database? Then try next API result or move on to Google searching.\n",
    "                print(\"  Error collecting URL from Google Places API. Moving on.\")\n",
    "                pass\n",
    "    \n",
    "    except:\n",
    "        print(\"  Google Places API search failed. Moving on to Google search.\")\n",
    "        pass\n",
    "    \n",
    "    \n",
    "\n",
    "    ## SECOND URL-SCRAPE ATTEMPT: FILTERED GOOGLE SEARCH\n",
    "    # Automate Google search and take first result that doesn't have a bad_sites_list element in it.\n",
    "    \n",
    "    \n",
    "    # Loop through google search output to find first good result:\n",
    "    try:\n",
    "        new_urls = list(search(search_terms, stop=numgoo, pause=wait_time))  # Grab first numgoo Google results (URLs)\n",
    "        print(\"  Successfully collected Google search results.\")\n",
    "        \n",
    "        for url in new_urls:\n",
    "            if any(domain in url for domain in bad_sites_list):\n",
    "                k+=1    # If this url is in bad_sites_list, add 1 to counter and move on\n",
    "                #print(\"  Bad site detected. Moving on.\")\n",
    "            else:\n",
    "                good_url = url\n",
    "                print(\"    Success! URL obtained by Google search with \" + str(k) + \" bad URLs avoided.\")\n",
    "                break    # Exit for loop after first good url is found\n",
    "                \n",
    "    \n",
    "    except:\n",
    "        print(\"  Problem with collecting Google search results. Try this by hand instead.\")\n",
    "            \n",
    "        \n",
    "    '''\n",
    "    # For testing/ debugging purposes:\n",
    "    \n",
    "    if k>2:  # Print warning messages depending on number of bad sites preceding good_url\n",
    "        print(\"  WARNING!! CHECK THIS URL!: \" + good_url + \\\n",
    "              \"\\n\" + str(k) + \" bad Google results have been omitted.\")\n",
    "    if k>1:\n",
    "        print(str(k) + \" bad Google results have been omitted. Check this URL!\")\n",
    "    elif k>0:\n",
    "        print(str(k) + \" bad Google result has been omitted. Check this URL!\")\n",
    "    else: \n",
    "        print(\"  No bad sites detected. Reliable URL!\")\n",
    "    \n",
    "    if manual_url != \"\":\n",
    "        if manual_url == good_url:\n",
    "            print(\"    Awesome! The known and discovered URLs are the SAME!\")\n",
    "    '''\n",
    "    \n",
    "    if good_url == \"\":\n",
    "        print(\"  WARNING! No good URL found via API or google search.\\n\")\n",
    "    \n",
    "    return(k + 1, good_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numschools = 0  # initialize scraping counter\n",
    "\n",
    "keys = sample[0].keys()  # define keys for writing function\n",
    "fname = \"../data/final_schools.csv\"  # define file name for writing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for school in sample[:2]:\n",
    "    print(school[\"URL\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now to call the above function and actually scrape these things!\n",
    "# \n",
    "for school in tqdm(sample): # loop through list of schools (sample)\n",
    "    if school[\"URL\"] == \"\":  # if URL is missing, fill that in by scraping\n",
    "        numschools += 1\n",
    "        school[\"QUERY_RANKING\"], school[\"URL\"] = getURL(school[\"SCH_NAME\"], school[\"ADDRESSES\"], bad_sites) # school[\"MANUAL_URL\"]\n",
    "    \n",
    "    else:\n",
    "        if school[\"URL\"]:\n",
    "            pass  # If URL exists, don't bother scraping it again\n",
    "\n",
    "        else:  # If URL hasn't been defined, then scrape it!\n",
    "            numschools += 1\n",
    "            school[\"QUERY_RANKING\"], school[\"URL\"] = \"\", \"\" # start with empty strings\n",
    "            school[\"QUERY_RANKING\"], school[\"URL\"] = getURL(school[\"SCH_NAME\"], school[\"ADDRESSES\"], bad_sites) # school[\"MANUAL_URL\"]\n",
    "\n",
    "print(\"\\n\\nURLs discovered for \" + str(numschools) + \" schools.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In summer 2017, the above approach works to get a good URL for 6,677 out of the 6,752 schools in this data set. Not bad! <br>\n",
    "For some reason, the Google search algorithm (method #2) is less likely to work after passing from the Google Places API. <br>\n",
    "To fill in for the remaining 75, let's skip the function's layers of code and just call the google search function by hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for school in sample:\n",
    "    school[\"SEARCH\"] = school[\"SCH_NAME\"] + \" \" + school[\"ADDRESSES\"]\n",
    "    if school[\"URL\"] == \"\":\n",
    "        k = 0  # initialize counter for number of URLs skipped\n",
    "        school[\"QUERY_RANKING\"] = \"\"\n",
    "\n",
    "        \n",
    "        print(\"Scraping URL for \" + school[\"SEARCH\"] + \"...\")\n",
    "        urls_list = list(search(school[\"SEARCH\"], stop=20, pause=10.0))\n",
    "        print(\"  URLs list collected successfully!\")\n",
    "\n",
    "        for url in urls_list:\n",
    "            if any(domain in url for domain in bad_sites):\n",
    "                k+=1    # If this url is in bad_sites_list, add 1 to counter and move on\n",
    "                # print(\"  Bad site detected. Moving on.\")\n",
    "            else:\n",
    "                good_url = url\n",
    "                print(\"    Success! URL obtained by Google search with \" + str(k) + \" bad URLs avoided.\")\n",
    "\n",
    "                school[\"URL\"] = good_url\n",
    "                school[\"QUERY_RANKING\"] = k + 1\n",
    "                \n",
    "                count_left(sample, 'URL')\n",
    "                dicts_to_csv(sample, fname, keys)\n",
    "                print()\n",
    "                break    # Exit for loop after first good url is found                               \n",
    "                                           \n",
    "    else:\n",
    "        pass\n",
    "\n",
    "count_left(sample, 'URL')\n",
    "dicts_to_csv(sample, fname, keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save sample to file (can continue to load and add to it):\n",
    "count_left(sample, 'URL')\n",
    "dicts_to_csv(sample, fname, keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHECK OUT RESULTS\n",
    "# TO DO: Make a histogram of 'NUM_BAD_URLS'\n",
    "# systematic way to look at problem URLs (with k > 0)?\n",
    "\n",
    "f = 0\n",
    "for school in sample:\n",
    "    if int(school['NUM_BAD_URLS']) > 14:\n",
    "        print(school[\"SEARCH\"], \"\\n\", school[\"URL\"], \"\\n\")\n",
    "        f += 1\n",
    "\n",
    "print(str(f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downloading and mirroring with `wget`<a id='wget'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limitations\n",
    "\n",
    "-only works for static HTML and it doesnâ€™t support JavaScript. Thus any element generated by JS will not be captured. \n",
    "\n",
    "More info:\n",
    "\n",
    "https://www.petekeen.net/archiving-websites-with-wget\n",
    "\n",
    "http://askubuntu.com/questions/411540/how-to-get-wget-to-download-exact-same-web-page-html-as-browser\n",
    "\n",
    "https://www.reddit.com/r/linuxquestions/comments/3tb7vu/wget_specify_dns_server/\n",
    "failed: nodename nor servname provided, or not known."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------------------------------------\n",
    "#Define most general wget parameters (more specific params below)\n",
    "#This list would not be so long if Parallel would allow wget to read from /usr/local/etc/wgetrc\n",
    "wget_general_options = '--no-parent --level 7 --no-check-certificate \\\n",
    "--recursive --adjust-extension --convert-links --page-requisites --wait=2 --random-wait \\\n",
    "-e --robots=off --follow-ftp --secure-protocol=auto --retry-connrefused --tries=12 --no-remove-listing \\\n",
    "--local-encoding=UTF-8 --no-cookies --default-page=default --server-response --trust-server-names \\\n",
    "--header=\"Accept:text/html\" --exclude-directories=' + exclude_dirs + reject_files\n",
    "#---------------------------------------------------------------\n",
    "\n",
    "#Other options:\n",
    "#--verbose --convert-file-only --force-directories --show-progress \n",
    "#--user_agent = Mozilla/5.0 (X11; Fedora; Linux x86_64; rv:40.0) Gecko/20100101 Firefox/40.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some of these options explained: \n",
    "'''\n",
    "--warc-file turns on WARC output to the specified file\n",
    "--warc-cdx tells wget to dump out an index file for our new WARC file\n",
    "--page-requisites will grab all of the linked resources necessary to render the page (images, css, javascript, etc)\n",
    "--adjust-extension appends .html to the files when appropriate\n",
    "--convert-links will turn links into local links as appropriate\n",
    "--execute robots=off turns off wget's automatic robots.txt checking\n",
    "--exclude-directories includes a comma-separated list of directories that wget should exclude in the archive\n",
    "--user-agent overrides wget's default user agent\n",
    "--random-wait will randomize that wait to between 5 and 15 seconds\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import os, csv\n",
    "import shutil\n",
    "import urllib\n",
    "from urllib.request import urlopen\n",
    "from socket import error as SocketError\n",
    "import errno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#setting directories\n",
    "micro_sample_cvs = \"/Users/anhnguyen/Desktop/research/scraping_Python/micro-sample_Feb17.csv\"\n",
    "wget_folder = \"/Users/anhnguyen/Desktop/research/scraping_Python/wget_accept\"\n",
    "no_dir_folder = \"/Users/anhnguyen/Desktop/research/scraping_Python/no_dir\"\n",
    "learning_wget = \"/Users/anhnguyen/Desktop/research/scraping_Python/learning_wget\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample = [] # make empty list\n",
    "with open(micro_sample_cvs, 'r', encoding = 'Windows-1252')\\\n",
    "as csvfile: # open file; the windows-1252 encoding looks weird but works for this\n",
    "    reader = csv.DictReader(csvfile) # create a reader\n",
    "    for row in reader: # loop through rows\n",
    "        sample.append(row) # append each row to the list\n",
    "        \n",
    "#note: each row, sample[i] is a dictionary with keys as column name and value as info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# turning this into tuples we can use with wget!\n",
    "# first, make some empty lists\n",
    "url_list = []\n",
    "name_list = []\n",
    "terms_list = []\n",
    "\n",
    "# now let's fill these lists with content from the sample\n",
    "for school in sample:\n",
    "    url_list.append(school[\"URL\"])\n",
    "    name_list.append(school[\"SCHNAM\"])\n",
    "    terms_list.append(school[\"ADDRESS\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('https://www.richland2.org/charterhigh/', 'RICHLAND TWO CHARTER HIGH'), ('https://www.polk.edu/lakeland-gateway-to-college-high-school/', 'POLK STATE COLLEGE COLLEGIATE HIGH SCHOOL'), ('https://www.nhaschools.com/schools/rivercity/Pages/default.aspx', 'RIVER CITY SCHOLARS CHARTER ACADEMY')]\n",
      "\n",
      " Polk State College Collegiate High School\n"
     ]
    }
   ],
   "source": [
    "tuple_list = list(zip(url_list, name_list))\n",
    "# Let's check what these tuples look like:\n",
    "print(tuple_list[:3])\n",
    "print(\"\\n\", tuple_list[1][1].title())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_parent_link(str):\n",
    "    \"\"\"Function to get parents' links. Return a list of valid links.\"\"\"\n",
    "    ls= get_parent_link_helper(5, str, []);\n",
    "    if len(ls) > 1:\n",
    "        return ls[0]\n",
    "    return str\n",
    "\n",
    "def get_parent_link_helper(level, str, result):\n",
    "    \"\"\"This is a tail recursive function\n",
    "    to get parent link of a given link. Return a list of urls \"\"\"\n",
    "    if level == 0 or not check(str):\n",
    "        return ''\n",
    "    else:\n",
    "        result += [str]\n",
    "        return get_parent_link_helper(num -1, str[: str.rindex('/')], result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_folder_name (k, name):\n",
    "    \"\"\"Format a folder nicely for easy access\"\"\"\n",
    "    if k < 10: # Add two zeros to the folder name if k is less than 10 (for ease of organizing the output folders)\n",
    "        dirname = \"00\" + str(k) + \" \" + name\n",
    "    elif k < 100: # Add one zero if k is less than 100\n",
    "        dirname = \"0\" + str(k) + \" \" + name\n",
    "    else: # Add nothing if k>100\n",
    "        dirname = str(k) + \" \" + name\n",
    "    return dirname\n",
    "\n",
    "def run_wget_command(link, parent_folder, my_folder):\n",
    "    \"\"\"wget on link and print output to appropriate folders\"\"\"\n",
    "    #navigate to parent folder\n",
    "    os.chdir(parent_folder)\n",
    "    # create dir my_folder if it doesn't exist yet\n",
    "    if not os.path.exists(my_folder):\n",
    "        os.makedirs(my_folder)\n",
    "    #navigate to the correct folder, ready to wget\n",
    "    os.chdir(my_folder)\n",
    "    os.system('wget --header=\"Accept: text/html\" -r --level=3 --accept .html --referer= '+get_parent_link(link) + ' ' + link)\n",
    "#     os.system('wget -np --no-parent --show-progress --progress=dot --recursive --level=3 --convert-links --retry-connrefused \\\n",
    "#          --random-wait --no-cookies --secure-protocol=auto --no-check-certificate --execute robots=off \\\n",
    "#          --header \"Accept: text/html\" \\\n",
    "#          --user-agent=\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36\" \\\n",
    "#           --accept .html' + ' ' + link)\n",
    "    \n",
    "\n",
    "def contains_html(my_folder):\n",
    "    \"\"\"check if a wget is success by checking if a directory has a html file\"\"\"\n",
    "\n",
    "    for r,d,f in os.walk(my_folder):\n",
    "        for file in f:\n",
    "            if file.endswith('.html'):\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "def count_with_file_ext(folder, ext):\n",
    "    count = 0\n",
    "    for r,d,f in os.walk(my_folder):\n",
    "        for file in f:\n",
    "            if file.endswith(ext):\n",
    "                count +=1\n",
    "    return count \n",
    "\n",
    "# write a file and add num line at the beginning of line\n",
    "def write_to_file(num, link, file_name):\n",
    "    with open(file_name, \"a\") as text_file:\n",
    "        text_file.write(str(num) + \"\\t\" + link +\"\\n\")\n",
    "\n",
    "# just write str to file\n",
    "def write_file(str, file_name):\n",
    "    with open(file_name, \"a\") as text_file:\n",
    "        text_file.write(str)\n",
    "        \n",
    "def reset(folder, text_file_1, text_file_2):\n",
    "    \"\"\"Deletes all files in a folder and set 2 text files to blank\"\"\"\n",
    "    parent_folder = folder[: folder.rindex('/')]\n",
    "    shutil.rmtree(folder)\n",
    "    os.makedirs(folder)\n",
    "    filelist = [ f for f in os.listdir(folder) if f.endswith(\".bak\") ]\n",
    "    for f in filelist:\n",
    "        os.unlink(f)\n",
    "    for file_name in [text_file_1, text_file_2]:\n",
    "        reset_text_file(file_name)\n",
    "        \n",
    "def reset_text_file(file_name):\n",
    "    if os.path.exists(file_name):\n",
    "            with open(file_name, \"w\") as text_file:\n",
    "                text_file.write(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "030 name me\n"
     ]
    }
   ],
   "source": [
    "#testing methods\n",
    "print(format_folder_name(30, \"name me\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check(url):\n",
    "    \"\"\" Helper function, check if url is a valid list\"\"\"\n",
    "    try:\n",
    "        urlopen(url)\n",
    "        \n",
    "    except urllib.error.URLError:\n",
    "        print(\"urllib.error.URLError\")\n",
    "        return False\n",
    "    except urllib.error.HTTPError:\n",
    "        print('urllib.error.HTTPError')\n",
    "        return False\n",
    "    except SocketError:\n",
    "        print('SocketError')\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def read_txt(txt_file):\n",
    "    links = []\n",
    "    count = 0\n",
    "    with open(txt_file) as f:\n",
    "        for line in f:   \n",
    "            \n",
    "            elem =  line.split('\\t')[1].rstrip()\n",
    "            count +=1\n",
    "    \n",
    "#             print(elem)\n",
    "            links += [elem.rstrip()]\n",
    "    return links, count\n",
    "\n",
    "def read_txt_2(txt_file):\n",
    "    links = []\n",
    "    count = 0\n",
    "    with open(txt_file) as f:\n",
    "        for line in f:   \n",
    "            \n",
    "#             elem =  line.split('\\t')[1].rstrip()\n",
    "#             if elem.endswith('\\'):\n",
    "#                 elem = elem[:-1]\n",
    "            count +=1\n",
    "    \n",
    "#             print(elem)\n",
    "            links += [line.rstrip()]\n",
    "    return links, count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set up file directories\n",
    "success_file = \"/Users/anhnguyen/Desktop/research/scraping_Python/success.txt\"\n",
    "fail_file = \"/Users/anhnguyen/Desktop/research/scraping_Python/fail.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_now = '/Users/anhnguyen/Desktop/research/scraping_Python/validlinks_from_Sammy.txt'\n",
    "list_valid_now,count = read_txt_2(valid_now)\n",
    "for link in list_valid_now:\n",
    "    run_wget_command(str(link), wget_folder, \"new \"+ str(link)[6:])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reset(wget_folder, success_file, fail_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "k=200 # initialize this numerical variable k, which keeps track of which entry in the sample we are on.\n",
    "\n",
    "#testing the first 10 tuples\n",
    "# tuple_test = tuple_list[200:300]\n",
    "\n",
    "\n",
    "for tup in tuple_test:\n",
    "    school_title = tup[1].title()\n",
    "\n",
    "\n",
    "    k += 1 # Add one to k, so we start with 1 and increase by 1 all the way up to entry # 300\n",
    "    print(\"Capturing website data for\", school_title + \", which is school #\" + str(k), \"of 300...\")\n",
    "    \n",
    "    # use the tuple to create a name for the folder\n",
    "    dirname = format_folder_name(k, school_title)\n",
    "    \n",
    "    run_wget_command(tup[0], wget_folder, dirname)\n",
    "    \n",
    "    school_folder = wget_folder + '/'+ dirname\n",
    "    if contains_html(school_folder):\n",
    "        write_file( tup[0], success_file )\n",
    "    else :\n",
    "        write_file( tup[0], fail_file)\n",
    "print(\"done!\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 243 links in success file.\n"
     ]
    }
   ],
   "source": [
    "success_links, count = read_txt(success_file)\n",
    "print(\"There are {} links in success file.\".format( count))\n",
    "# print(success_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 57 links in fail file.\n"
     ]
    }
   ],
   "source": [
    "fail_links, count = read_txt(fail_file)\n",
    "print(\"There are {} links in fail file.\".format( count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counting # of html files\n",
    "# def count_html(file):\n",
    "    \n",
    "def count_valid_links(list_of_links, valid_file, invalid_file):\n",
    "    count_success, count_fail = 0, 0\n",
    "    valid, invalid = '', ''\n",
    "    for l in list_of_links:\n",
    "#         print(l)\n",
    "        if check(l):\n",
    "            valid += l + '\\n'\n",
    "            count_success +=1\n",
    "        else:\n",
    "            invalid += l + '\\n'\n",
    "            count_fail += 1\n",
    "#             print(l)\n",
    "    write_file(valid, valid_file)\n",
    "    write_file(invalid, invalid_file)\n",
    "    return count_success, count_fail\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_list = '/Users/anhnguyen/Desktop/research/scraping_Python/valid_links.txt'\n",
    "invalid_list = '/Users/anhnguyen/Desktop/research/scraping_Python/invalid_links.txt'\n",
    "reset_text_file(valid_list)\n",
    "reset_text_file(invalid_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "urllib.error.URLError\n",
      "urllib.error.URLError\n",
      "urllib.error.URLError\n",
      "urllib.error.URLError\n",
      "urllib.error.URLError\n",
      "urllib.error.URLError\n",
      "urllib.error.URLError\n",
      "urllib.error.URLError\n",
      "urllib.error.URLError\n",
      "urllib.error.URLError\n",
      "urllib.error.URLError\n",
      "urllib.error.URLError\n",
      "urllib.error.URLError\n",
      "urllib.error.URLError\n",
      "urllib.error.URLError\n",
      "urllib.error.URLError\n",
      "urllib.error.URLError\n",
      "urllib.error.URLError\n",
      "urllib.error.URLError\n",
      "urllib.error.URLError\n",
      "urllib.error.URLError\n",
      "urllib.error.URLError\n",
      "urllib.error.URLError\n",
      "urllib.error.URLError\n",
      "urllib.error.URLError\n",
      "urllib.error.URLError\n"
     ]
    }
   ],
   "source": [
    "\n",
    "count_success, count_fail = count_valid_links(fail_links, valid_list, invalid_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 31 valid links and 26 invalid links\n"
     ]
    }
   ],
   "source": [
    "print(\"There are {} valid links and {} invalid links\".format(count_success, count_fail))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26\n"
     ]
    }
   ],
   "source": [
    "# recheck links without \"/\"\n",
    "recheck, count = read_txt_2(invalid_list)\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://responsiveed.com/dallasclassical\n"
     ]
    }
   ],
   "source": [
    "for index in range (0, len(recheck)):\n",
    "    if recheck[index].endswith('/'):\n",
    "        recheck[index] = recheck[index][: recheck[index].rindex('/')]\n",
    "print(recheck[20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.trinityschoolforchildren.org\n",
      "http://www.pasadenarosebud.com\n",
      "http://www.mlacademy.org/#!contact-us/c2q4\n",
      "http://www.materacademy.com/schools\n",
      "http://www.jeffersoncommunityschool.org\n",
      "http://www.evergladesprep.com/pages/Everglades_Preparatory_Academy\n",
      "http://www.clevelandta.org/school/oak-leadership-institute\n",
      "http://www.chandlerparkacademy.net/index.php/schools/elementary-school.html\n",
      "http://www.ccaschool.net\n",
      "http://www.blracademy.org\n",
      "http://www.academycharterhs.org/pages/mainpg\n",
      "http://www.academiadeestrellas.org\n",
      "http://rpes-susd-ca.schoolloop.com\n",
      "http://responsiveed.com/premierpharrmcallen\n",
      "http://responsiveed.com/premiernewbraunfels\n",
      "http://responsiveed.com/huntsvilleclassical\n",
      "http://responsiveed.com/dallasclassical\n",
      "http://ideacharterschool.com\n",
      "http://gowan.craneschools.org\n",
      "http://arthuracademy.org/woodburn/woodburn-arthur-academy.html\n"
     ]
    }
   ],
   "source": [
    "invalid2 = '/Users/anhnguyen/Desktop/research/scraping_Python/invalid2.txt'\n",
    "count_success, count_fail = count_valid_links(recheck, valid_list, invalid2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 6 valid links and 20 invalid links\n"
     ]
    }
   ],
   "source": [
    "print(\"There are {} valid links and {} invalid links\".format(count_success, count_fail))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Runing wget with log output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# setting up files\n",
    "invalid2 = '/Users/anhnguyen/Desktop/research/scraping_Python/invalid2.txt'\n",
    "log = '/Users/anhnguyen/Desktop/research/scraping_Python/wget_accept_logs.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "failed_links, counts = read_txt(invalid2)\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "urllib.error.URLError\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "## something wrong with check function???\n",
    "print(check('http://responsiveed.com/dallasclassical'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.chdir('/Users/anhnguyen/Desktop/research/scraping_Python/no_dir')\n",
    "reset_text_file(log)\n",
    "for link in failed_links:\n",
    "    \n",
    "    \n",
    "    os.system('wget -np --no-parent --show-progress --progress=dot --recursive --level=3 --convert-links --retry-connrefused --tries=5\\\n",
    "         --random-wait --no-cookies --secure-protocol=auto --no-check-certificate --execute robots=off \\\n",
    "         --header \"Host: jrs-s.net\" \\\n",
    "         --output-file=log \\\n",
    "         --user-agent=\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36\" \\\n",
    "          --accept .html' + ' ' + link)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
